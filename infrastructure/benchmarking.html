<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Benchmarking &mdash; ActivitySim 1.1.0 documentation</title>
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Software Development" href="development.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> ActivitySim
          </a>
              <div class="version">
                1.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gettingstarted.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="core.html">Core Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">Command Line Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="estimation.html">Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="howitworks.html">How the System Works</a></li>
<li class="toctree-l1"><a class="reference internal" href="development.html">Software Development</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Benchmarking</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#benchmarking-setup">Benchmarking Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-benchmarks">Running Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#threading-limits">Threading Limits</a></li>
<li class="toctree-l2"><a class="reference internal" href="#submitting-benchmarks">Submitting Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#publishing-to-github-pages">Publishing to Github Pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="#profiling">Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#writing-new-benchmarks">Writing New Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-benchmarks-for-pull-requests">Running Benchmarks for Pull Requests</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ActivitySim</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Benchmarking</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/benchmarking.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="benchmarking">
<span id="id1"></span><h1>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h1>
<p>A key focus of the ActivitySim project is <em>performance</em>.  It’s not enough
to build a new modeling platform that’s mathematically sound and simulates
travel behavior as expected.  It’s also required that it do so quickly.
It’s not too hard to run performance tests manually on individual models, and
doing so after making changes that are expected to <em>improve</em> performance is
typical. But monitoring performance regularly and automatically can help ensure
that new features do not introduce unexpected performance regressions (i.e.
models run slower than before). Developing an extensive set of automatic
performance benchmarks can streamline the former problem and solve the latter.</p>
<p>ActivitySim includes the ability to run performance benchmarks using a tool
called <a class="reference external" href="https://asv.readthedocs.io/en/stable/">airspeed velocity</a>.</p>
<p>The benchmarking process is closely tied to ActivitySim’s <em>git</em> repository,
so it is recommended that you use Git to clone the repository from GitHub.</p>
<section id="benchmarking-setup">
<h2>Benchmarking Setup<a class="headerlink" href="#benchmarking-setup" title="Permalink to this headline">¶</a></h2>
<p>The first step in running benchmarks is to have a conda environment for
benchmarking, as well as a local clone of the main ActivitySim repository,
plus one of the <code class="docutils literal notranslate"><span class="pre">asim-benchmarks</span></code> repository. If you plan to submit your
benchmarking results to the common repository of results, you’ll want to
also make sure that your <code class="docutils literal notranslate"><span class="pre">asim-benchmarks</span></code> repository is using a fork of the
common repository to which you have write-access.</p>
<p>If this isn’t already set up on your performance benchmarking machine, you can
do all of this setup by following these steps:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="n">ASIM</span><span class="o">-</span><span class="n">BENCH</span> <span class="n">mamba</span> <span class="n">git</span> <span class="n">gh</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="o">--</span><span class="n">override</span><span class="o">-</span><span class="n">channels</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="n">ASIM</span><span class="o">-</span><span class="n">BENCH</span>
<span class="n">gh</span> <span class="n">auth</span> <span class="n">login</span>  <span class="c1"># &lt;--- (only needed if gh is not logged in)</span>
<span class="n">gh</span> <span class="n">repo</span> <span class="n">clone</span> <span class="n">ActivitySim</span><span class="o">/</span><span class="n">activitysim</span>          <span class="c1"># TEMPORARY: use jpn--/activitysim</span>
<span class="n">cd</span> <span class="n">activitysim</span>
<span class="n">git</span> <span class="n">switch</span> <span class="n">develop</span>                             <span class="c1"># TEMPORARY: use performance1 branch</span>
<span class="n">mamba</span> <span class="n">env</span> <span class="n">update</span> <span class="o">--</span><span class="n">file</span><span class="o">=</span><span class="n">conda</span><span class="o">-</span><span class="n">environments</span><span class="o">/</span><span class="n">activitysim</span><span class="o">-</span><span class="n">dev</span><span class="o">.</span><span class="n">yml</span>
<span class="n">cd</span> <span class="o">..</span>
<span class="n">gh</span> <span class="n">repo</span> <span class="n">fork</span> <span class="n">ActivitySim</span><span class="o">/</span><span class="n">asim</span><span class="o">-</span><span class="n">benchmarks</span> <span class="o">--</span><span class="n">remote</span>
<span class="n">cd</span> <span class="n">asim</span><span class="o">-</span><span class="n">benchmarks</span>
<span class="n">python</span> <span class="n">initialize</span><span class="o">-</span><span class="n">hooks</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>For non-Windows users, you can then actually activate the pre-commit hooks like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pre</span><span class="o">-</span><span class="n">commit</span> <span class="n">install</span>     <span class="c1"># macOS/Linux only, do not run this line on Windows</span>
</pre></div>
</div>
<p>Windows users should not attempt to use installed pre-commit hooks with conda
(see note below).  Instead, you must manually <code class="docutils literal notranslate"><span class="pre">pre-commit</span> <span class="pre">run</span></code> inside the correct
conda environment before committing.</p>
<p>If this environment is set up but it’s been a while since you last used it,
consider updating the environment like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">ASIM</span><span class="o">-</span><span class="n">BENCH</span>
<span class="n">cd</span> <span class="n">activitysim</span>
<span class="n">git</span> <span class="n">switch</span> <span class="n">develop</span>                             <span class="c1"># TEMPORARY: use performance1 branch</span>
<span class="n">mamba</span> <span class="n">env</span> <span class="n">update</span> <span class="o">--</span><span class="n">file</span><span class="o">=</span><span class="n">conda</span><span class="o">-</span><span class="n">environments</span><span class="o">/</span><span class="n">activitysim</span><span class="o">-</span><span class="n">dev</span><span class="o">.</span><span class="n">yml</span>
<span class="n">cd</span> <span class="o">..</span>
<span class="n">cd</span> <span class="n">asim</span><span class="o">-</span><span class="n">benchmarks</span>
<span class="n">git</span> <span class="n">pull</span>
</pre></div>
</div>
<p>Next, we’ll want to declare the specs of our benchmarking machine.  Some of
these can be determined quasi-automatically, but we want to confirm the specs
we’ll use as they are written with our benchmark results into the database.
Define machine specs by running this command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">machine</span>
</pre></div>
</div>
<p>This will start an interactive questions and answer session to describe your
computer.  Don’t be afraid, just answer the questions.  The tool may make
suggestions, but they are not always correct, so check them first and don’t just
accept all.  For example, under “arch” it may suggest “AMD64”, but for consistency
you can change that to “x86_64”, which is the same thing by a different name.</p>
</section>
<section id="running-benchmarks">
<h2>Running Benchmarks<a class="headerlink" href="#running-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>ActivitySim automates the process of running many benchmarks. It can also easily
accumulate and analyze benchmark results across many different machines, as long as the
benchmarks are all run in the same (relative) place. So before running benchmarks,
change your working directory (at the command prompt) into the top directory of
the <cite>asim-benchmarks</cite> repository, if you’re not already there.</p>
<p>To run all of the benchmarks on the most recent commit in the main ActivitySim repo:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">latest</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The benchmarks do not currently use ActivitySim’s dynamic chunking features,
as these require manual configuration and training on a per-machine basis
to ensure good performance.</p>
<p>Running the complete suite of benchmarks currently includes downloading and
running full-region model data for several different SANDAG zone systems.
Ideally you should have at least 50 GB of free disk space and 120 GB of RAM
to attempt this process on any given machine.  For a smaller machine, consider
benchmarking only the “test” sized examples, by adding <cite>–bench sandag.example</cite>
to this command, as discussed below.</p>
</div>
<p>This will run the benchmarks only on the “HEAD” commit of the main activitysim git
repository.  To run on some other historical commit[s] from the git history, you can
specify an individual commit or a range, in the same way you would do so for the
<cite>git log</cite> command. For example, to run benchmarks on the commits to develop since
it was branched off master, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">run</span> <span class="n">master</span><span class="o">..</span><span class="n">develop</span>
</pre></div>
</div>
<p>or to run only on the latest commit in develop, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">run</span> <span class="s2">&quot;develop^!&quot;</span>
</pre></div>
</div>
<p>Note that the literal quotation marks are necessary on Windows, as the carat character
preceding the exclamation mark is otherwise interpreted as an escape character.
In most other shells (e.g. on Linux or macOS) the literal quotation marks are unnecessary.</p>
<p>To run only benchmarks from a certain example, we can
use the <cite>–bench</cite> argument, which allows us to write a “regular expression” that
filters the benchmarks actually executed.  This is handy if you are interested in
benchmarking a particular model or component, as running <em>all</em> the benchmarks can
take a very long time, and the larger benchmarks (e.g. on the full SANDAG model)
will need a lot of disk space and RAM.  For example, to run only the mandatory
tour frequency benchmark for the SANDAG 1-Zone example-sized system, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">latest</span> <span class="o">--</span><span class="n">bench</span> <span class="n">sandag1example</span><span class="o">.</span><span class="n">time_mandatory_tour_frequency</span>
</pre></div>
</div>
<p>The “.” character here means a literal dot, but since this is a regex expression,
it is also a single-character wildcard.  Thus, you can run all the example-sized
SANDAG benchmarks with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">latest</span> <span class="o">--</span><span class="n">bench</span> <span class="n">sandag</span><span class="o">.</span><span class="n">example</span>
</pre></div>
</div>
<p>You can also repeat the <cite>–bench</cite> argument to give multiple different expressions.
So, you can run just the 1- and 2-zone examples, without the 3-zone example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">latest</span> <span class="o">--</span><span class="n">bench</span> <span class="n">sandag1example</span> <span class="o">--</span><span class="n">bench</span> <span class="n">sandag2example</span>
</pre></div>
</div>
<p>If you want to run several different benchmarking commmands together, for example
to run a custom curated subset of interesting benchmarks, the benchmark tool also
includes a <cite>batch</cite> mode.  You can assemble the various commands you would run
(i.e. everything you would type on the command line after “activitysim benchmark”)
into a text file, and then point to that file using the <cite>batch</cite> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">batch</span> <span class="n">my_interesting_benchmarks</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
<section id="threading-limits">
<h2>Threading Limits<a class="headerlink" href="#threading-limits" title="Permalink to this headline">¶</a></h2>
<p>When you run benchmarking using the <cite>activitysim benchmark</cite> command, the
following environment variable are set automatically before benchmarking begins:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MKL_NUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">OMP_NUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">OPENBLAS_NUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">NUMBA_NUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">VECLIB_MAXIMUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">NUMEXPR_NUM_THREADS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
<p>This ensures that all benchmarking operations run processes in single-threaded
mode.  This still allows ActivitySim itself to spin up multiple processes if the
item being timed is a multiprocess benchmark.</p>
</section>
<section id="submitting-benchmarks">
<h2>Submitting Benchmarks<a class="headerlink" href="#submitting-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>One of the useful features of the airspeed velocity benchmarking engine is the
opportunity to compare performance benchmarks across different machines. The
ActivitySim community is interested in aggregating such results from a number
of participants, so once you have successfully run a set of benchmarks, you
should submit those results to our repository.</p>
<p>To do so, assuming you have run the benchmark tool inside the <code class="docutils literal notranslate"><span class="pre">asim-benchmarks</span></code>
repository as noted above, you simply need to commit any new or changed files
in the <code class="docutils literal notranslate"><span class="pre">asim-benchmarks/results</span></code> directory.  You can then open a pull request
against the community <code class="docutils literal notranslate"><span class="pre">asim-benchmarks</span></code> to submit those results.</p>
<p>Assuming you are in (or first <code class="docutils literal notranslate"><span class="pre">cd</span></code> into) the <code class="docutils literal notranslate"><span class="pre">asim-benchmarks</span></code> directory, You can
do this from the command line using the following steps:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">add</span> <span class="n">results</span>
<span class="n">pre</span><span class="o">-</span><span class="n">commit</span> <span class="n">run</span>    <span class="c1"># required on Windows only, see note</span>
<span class="n">git</span> <span class="n">commit</span> <span class="o">-</span><span class="n">m</span> <span class="s2">&quot;adding benchmark results&quot;</span>
<span class="n">git</span> <span class="n">push</span>
<span class="n">gh</span> <span class="n">pr</span> <span class="n">create</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>On Windows, the process for automatically running pre-commit hooks when
making a Git a commit is not compatible with conda, see
<cite>here &lt;https://github.com/pre-commit/pre-commit/issues/1329&gt;</cite>. This will
probably never be fixed, as the developers of pre-commit and conda each
feel that the “bug” is in the other library.  So, manually running the
pre-commit step is required.</p>
</div>
<p>Users may find it simpler to skip the last step on the command line, and simply
visit their fork on GitHub.com to use the web interface to open a pull request.</p>
</section>
<section id="publishing-to-github-pages">
<h2>Publishing to Github Pages<a class="headerlink" href="#publishing-to-github-pages" title="Permalink to this headline">¶</a></h2>
<p>Publishing the standard airspeed velocity content to GitHub pages is a built-in
feature of the command line tool, available to users who have write-access to the
asim-benchmarks GitHub repository.  Be sure you have all the relevant branches
tracked locally (especially master and develop) and then run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">gh</span><span class="o">-</span><span class="n">pages</span>
</pre></div>
</div>
</section>
<section id="profiling">
<h2>Profiling<a class="headerlink" href="#profiling" title="Permalink to this headline">¶</a></h2>
<p>The benchmarking tool can also be used for profiling, which allows a developer to
inspect the timings for various commands <em>inside</em> a particular benchmark. This is
most conveniently accomplished using the <code class="docutils literal notranslate"><span class="pre">snakeviz</span></code> tool, which should be installed
in the developer tools environment (<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">snakeviz</span> <span class="pre">-c</span> <span class="pre">conda-forge</span></code>).
Then, the developer needs to run two commands to compute and view the component
profile.</p>
<p>To create a profile record when benchmarking, add the <code class="docutils literal notranslate"><span class="pre">--profile</span></code> option when
running the benchmarks.  For example, to create profile records for the SANDAG
example-sized model’s non-mandatory tour scheduling component across all three
zone systems, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">latest</span> <span class="o">--</span><span class="n">bench</span> <span class="n">sandag</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">non_mandatory_tour_scheduling</span> <span class="o">--</span><span class="n">profile</span>
</pre></div>
</div>
<p>This command will save the profiling data directly into the json file that stores
the benchmark timings.  This is a lot of extra data, so it’s not advised to
save profiling data for every benchmark, but only for benchmarks of particular
interest.</p>
<p>Once this data has been saved, you can access it using the <code class="docutils literal notranslate"><span class="pre">snakeviz</span></code> tool.  This
visualization requires pointing to a specific profiled benchmark in a specific
json result file.  For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">snakeviz</span> <span class="n">results</span><span class="o">/</span><span class="n">LUMBERJACK</span><span class="o">/</span><span class="mi">241</span><span class="n">ddb64</span><span class="o">-</span><span class="n">env</span><span class="o">-</span><span class="n">c87ac846ee78e51351a06682de5adcb5</span><span class="o">.</span><span class="n">json</span> <span class="n">sandag3example</span><span class="o">.</span><span class="n">non_mandatory_tour_scheduling</span><span class="o">.</span><span class="n">time_component</span>
</pre></div>
</div>
<p>On running this command, a web browser should pop open to display the snakeviz
interface.</p>
</section>
<section id="writing-new-benchmarks">
<h2>Writing New Benchmarks<a class="headerlink" href="#writing-new-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>New benchmarks for other model examples can be added to
<code class="docutils literal notranslate"><span class="pre">activitysim/benchmarking/benchmarks</span></code>. A basic template structure has been used,
so that it should be relatively straight-forward to implement component-level
single thread benchmarks for any model that is available using the
<code class="docutils literal notranslate"><span class="pre">activitysim</span> <span class="pre">create</span></code> tool.</p>
<p>A basic framework for multi-processing benchmarks has been implemented and is
demonstrated in the <code class="docutils literal notranslate"><span class="pre">mtc1mp4</span></code> benchmark file. However, work remains to write
a stable process to execute chunking training for each machine prior to running
the production-version benchmarks that will be meaningful for users.</p>
</section>
<section id="running-benchmarks-for-pull-requests">
<h2>Running Benchmarks for Pull Requests<a class="headerlink" href="#running-benchmarks-for-pull-requests" title="Permalink to this headline">¶</a></h2>
<p>The complete set of performance benchmarks is too large to include in ActivitySim’s
automatic continuous integration (CI) testing, both by compute time and by memory usage.
However, it is valuable to run these tests once against the final version of
each PR before merging into the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch, to ensure there are no
unexpected performance regressions. The airspeed velocity tools include a special
CI mode, which runs the same benchmarks on the same machine with the same settings,
giving developers a fair shot at a strict apples-to-apples comparison of performance.</p>
<p>This mode can be activated to check the performance of code on a git branch called
<code class="docutils literal notranslate"><span class="pre">my-new-feature-branch</span></code>, and compare against the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">activitysim</span> <span class="n">benchmark</span> <span class="n">continuous</span> <span class="n">develop</span> <span class="n">my</span><span class="o">-</span><span class="n">new</span><span class="o">-</span><span class="n">feature</span><span class="o">-</span><span class="n">branch</span>
</pre></div>
</div>
<p>Unlike other tests for mathematical correctness, it is not always necessary that
new PR’s must “pass” this testing, as new features or capabilities may justify a
performance degradation.  But developers should always run these tests on new PR’s
so that the community is aware of the trade offs (if any) and can take steps to
mitigate problems promptly if desired.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="development.html" class="btn btn-neutral float-left" title="Software Development" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright contributing authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>